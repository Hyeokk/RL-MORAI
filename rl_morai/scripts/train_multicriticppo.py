#!/usr/bin/env python3
"""
Multi-Critic PPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ê°„ê²° ë²„ì „)
"""

import sys
import os
import argparse

# ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from Multi_PPO import MultiCriticPPO
from utils import TrainingSession, EnvironmentManager

def main():
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='Multi-Critic PPO Training')
    parser.add_argument('--environment', '-e', type=str, default='auto',
                       choices=['auto', 'solid', 'dashed', 'dash', 'shadow'],
                       help='í•™ìŠµí•  í™˜ê²½ ì§€ì •')
    parser.add_argument('--episodes', type=int, default=None,
                       help='í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ê°’: ëª¨ë¸ ë‚´ ì„¤ì •)')
    parser.add_argument('--save_dir', type=str, default="/home/kuuve/catkin_ws/src/pt/",
                       help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--log_dir', type=str, default=None,
                       help='ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    # í™˜ê²½ ì„¤ì •
    manual_env = args.environment if args.environment != 'auto' else None
    
    # í•™ìŠµ ì„¸ì…˜ ì´ˆê¸°í™”
    session = TrainingSession(
        manual_env=manual_env,
        save_dir=args.save_dir,
        log_dir=args.log_dir
    )
    
    session.print_experiment_info()
    
    try:
        # MORAI í™˜ê²½ ì´ˆê¸°í™”
        action_bounds = [(-0.4, 0.4), (15.0, 25.0)]
        env, sensor = EnvironmentManager.setup_environment(action_bounds)
        
        # Multi-Critic PPO ì—ì´ì „íŠ¸ ìƒì„±
        agent = MultiCriticPPO()
        
        # ì—í”¼ì†Œë“œ ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ
        if args.episodes:
            agent.num_episodes = args.episodes
        
        # ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        session.setup_logging(agent)
        session.env_manager.print_mode_info()
        
        # ì´ˆê¸° ê´€ì¸¡ í™•ì¸
        obs_dict, _ = env.reset()
        if obs_dict is None:
            print("[ERROR] í™˜ê²½ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
        
        print(f"ğŸ“‹ í•™ìŠµ ì„¤ì •:")
        print(f"   - ì´ ì—í”¼ì†Œë“œ: {agent.num_episodes}")
        print(f"   - ì—…ë°ì´íŠ¸ ê°„ê²©: {agent.update_interval} steps")
        print(f"   - ìµœëŒ€ ìŠ¤í…/ì—í”¼ì†Œë“œ: {agent.max_steps_per_episode}")
        print(f"   - ìµœì†Œ ì—í”¼ì†Œë“œ ê¸¸ì´: {agent.min_episode_steps}")
        print()
        
        # =================================================================
        # ë©”ì¸ í•™ìŠµ ë£¨í”„
        # =================================================================
        for episode in range(agent.num_episodes):
            if session.should_stop():
                break
            
            # ì—í”¼ì†Œë“œ ì´ˆê¸°í™”
            obs_dict, _ = env.reset()
            if obs_dict is None:
                continue
            
            agent.episode_reset()
            total_reward = 0
            episode_steps = 0
            
            # ì—í”¼ì†Œë“œ ì‹¤í–‰
            for step in range(agent.max_steps_per_episode):
                if session.should_stop():
                    break
                
                # í™˜ê²½ ë¼ë²¨ íšë“
                env_label = session.env_manager.get_environment_label(obs_dict, sensor)
                
                # ì•¡ì…˜ ì„ íƒ
                action, log_prob, value, detected_env = agent.get_action(obs_dict, training=True)
                
                # í™˜ê²½ ìŠ¤í…
                next_obs_dict, reward, done, _, _ = env.step(action)
                if next_obs_dict is None:
                    break
                
                # ê²½í—˜ ì €ì¥
                is_short_episode = agent.store_experience(
                    obs_dict, action, reward, 
                    value[0] if value is not None else 0.0, 
                    log_prob[0] if log_prob is not None else 0.0, 
                    done, next_obs_dict if not done else None, 
                    env_label, detected_env
                )
                
                # ì§§ì€ ì—í”¼ì†Œë“œ ì¡°ê¸° ì¢…ë£Œ
                if is_short_episode and done:
                    session.stats.add_short_episode(episode_steps + 1)
                    break
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                obs_dict = next_obs_dict
                total_reward += reward
                episode_steps += 1
                session.stats.total_steps += 1
                
                # PPO ì—…ë°ì´íŠ¸
                if session.stats.total_steps % agent.update_interval == 0:
                    print(f"[UPDATE] Step {session.stats.total_steps}ì—ì„œ PPO ì—…ë°ì´íŠ¸ ìˆ˜í–‰")
                    train_metrics = agent.train()
                    if train_metrics:
                        print(f"  ì†ì‹¤ - Actor: {train_metrics['actor_loss']:.4f}, "
                              f"Critic: {train_metrics['critic_loss']:.4f}, "
                              f"Classifier: {train_metrics['classifier_loss']:.4f}")
                        
                        # í•™ìŠµ ì§€í‘œ ë¡œê¹…
                        session.log_training_metrics(session.stats.total_steps, train_metrics, manual_env)
                
                if done:
                    break
            
            # ì—í”¼ì†Œë“œ í›„ì²˜ë¦¬
            if session.stats.is_invalid_episode(episode_steps, agent.min_episode_steps):
                if episode_steps > 1:
                    session.stats.add_short_episode(episode_steps)
                
                print(f"ë¬´íš¨ ì—í”¼ì†Œë“œ! Episode {episode+1}: {episode_steps}ìŠ¤í… "
                      f"(ì—°ì† {session.stats.consecutive_short_episodes}íšŒ)")
                
                # í™˜ê²½ ê°•ì œ ë¦¬ì…‹ (ì—°ì† ì‹¤íŒ¨ ì‹œ)
                if session.stats.consecutive_short_episodes >= 5:
                    env, sensor = EnvironmentManager.force_reset_environment(env, action_bounds)
                    agent.buffer.clear()
                    session.stats.total_steps = max(0, session.stats.total_steps - episode_steps)
                    session.stats.reset_consecutive_count()
                continue
            else:
                session.stats.reset_consecutive_count()
                
                # ì •ìƒ ì—í”¼ì†Œë“œ ê¸°ë¡
                final_env_label = session.env_manager.get_environment_label(obs_dict, sensor)
                session.stats.add_episode(total_reward, episode_steps, final_env_label)
                
                env_name = session.env_manager.get_environment_name(final_env_label)
                print(f"Episode {episode+1:4d}: Steps={episode_steps:3d}, "
                      f"Reward={total_reward:7.2f}, Env={env_name}")
                
                if manual_env:
                    env_label_num = session.env_manager.env_name_to_label.get(manual_env.lower(), 0)
                    print(f"  â†’ í•™ìŠµ í™˜ê²½: {manual_env} (Critic_{env_label_num})")
                
                # ì—í”¼ì†Œë“œ ì§€í‘œ ë¡œê¹…
                session.log_episode_metrics(episode+1, total_reward, episode_steps, final_env_label, manual_env)
            
            # ì£¼ê¸°ì  ë¡œê¹…
            if episode % agent.log_interval == 0:
                session.stats.print_stats(episode+1, manual_env)
            
            # ì£¼ê¸°ì  í‰ê°€
            if episode % agent.eval_interval == 0 and episode > 0:
                session.analyze_performance()
            
            # ëª¨ë¸ ì €ì¥
            if (episode + 1) % agent.save_interval == 0:
                save_path = session.get_save_path()
                agent.save_model(save_path)
                print(f"[SAVE] ëª¨ë¸ ì €ì¥: {save_path}")
        
        # =================================================================
        # í•™ìŠµ ì™„ë£Œ ì²˜ë¦¬
        # =================================================================
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_save_path = session.get_save_path(final=True)
        agent.save_model(final_save_path)
        
        # ìµœì¢… ìš”ì•½ ì¶œë ¥
        session.print_final_summary(episode + 1)
        
    except KeyboardInterrupt:
        print("\ní•™ìŠµì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\ní•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        session.cleanup(env)

if __name__ == "__main__":
    main()