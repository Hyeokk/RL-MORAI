import sys
import os
import time
import signal
import numpy as np
import torch
import random

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from gym_morai.envs.morai_env import MoraiEnv
from src.reward_fns import RewardFns
from src.terminated_fns import TerminatedFns
from models.SAC import SACAgent
from src.reward_fns import CurriculumReward
from src.terminated_fns import CurriculumTermination

# ì„¤ì •
SAVE_DIR = "/home/kuuve/catkin_ws/src/pt/"
os.makedirs(SAVE_DIR, exist_ok=True)

# ì‹œë“œ ì„¤ì •
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# ì¢…ë£Œ í”Œë˜ê·¸
stop_flag = False
def signal_handler(sig, frame):
    global stop_flag
    stop_flag = True
signal.signal(signal.SIGINT, signal_handler)

def force_reset_environment(env, action_bounds):
    """í™˜ê²½ ê°•ì œ ì¬ì´ˆê¸°í™”"""
    try:
        print("[RESET] í™˜ê²½ ê°•ì œ ì¬ì´ˆê¸°í™” ì¤‘...")
        env.close()
        time.sleep(2)
        
        new_env = MoraiEnv(action_bounds=action_bounds)
        sensor = new_env.sensor
        new_env.set_reward_fn(RewardFns.adaptive_speed_lanefollow_reward(sensor))
        new_env.set_episode_over_fn(TerminatedFns.cte_done(sensor, max_cte=0.8))  # ì¢€ ë” ì™„í™”
        
        print("[RESET] í™˜ê²½ ì¬ì´ˆê¸°í™” ì™„ë£Œ")
        return new_env
    except Exception as e:
        print(f"[ERROR] í™˜ê²½ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return env

def main():
    global stop_flag
    
    # ğŸ”§ ë³´ìˆ˜ì  ì„¤ì • (ì¡°í–¥ê° ì¶•ì†Œ, CTE ì™„í™”)
    action_bounds = [(-0.4, 0.4), (15.0, 25.0)]  # ì¡°í–¥ê° ì¶•ì†Œ
    env = MoraiEnv(action_bounds=action_bounds)
    
    sensor = env.sensor
    env.set_reward_fn(RewardFns.adaptive_speed_lanefollow_reward(sensor))
    env.set_episode_over_fn(TerminatedFns.cte_done(sensor, max_cte=0.8))  # 0.8 â†’ 1.5 ì™„í™”

    # ì´ˆê¸° ê´€ì¸¡
    obs_dict, _ = env.reset()
    if obs_dict is None:
        print("[ERROR] í™˜ê²½ ì´ˆê¸°í™” ì‹¤íŒ¨")
        sys.exit(1)
    
    image_shape = obs_dict['image'].shape[:2]  # (120, 160)
    vector_dim = len(obs_dict['velocity']) + len(obs_dict['steering'])  # 2

    # SAC ì—ì´ì „íŠ¸ ìƒì„±
    agent = SACAgent((120, 160), 2, action_bounds)
    
    num_episodes = 10000
    max_steps_per_episode = 10000
    
    # Step 1 ë°©ì–´ ë³€ìˆ˜
    consecutive_step1 = 0
    total_step1_count = 0

    # í•™ìŠµ ë£¨í”„
    for episode in range(num_episodes):
        if stop_flag:
            break

        obs_dict, _ = env.reset()
        if obs_dict is None:
            continue
            
        total_reward = 0
        episode_steps = 0
        episode_data = []  # ì—í”¼ì†Œë“œ ë°ì´í„° ì„ì‹œ ì €ì¥
        
        for step in range(max_steps_per_episode):
            if stop_flag:
                break
                
            action = agent.get_action(obs_dict, training=True)
            next_obs_dict, reward, done, _, _ = env.step(action)
            
            if next_obs_dict is None:
                break
                
            # ë°ì´í„°ë¥¼ ì„ì‹œë¡œ ì €ì¥ (ë°”ë¡œ replay bufferì— ë„£ì§€ ì•ŠìŒ)
            episode_data.append((obs_dict, action, reward, next_obs_dict, float(done)))
            
            obs_dict = next_obs_dict
            total_reward += reward
            episode_steps += 1
            #env.render()
            
            if done:
                break

        # Step 1 ë°©ì–´ ë¡œì§
        if episode_steps == 1:
            consecutive_step1 += 1
            total_step1_count += 1
            
            print(f"Step 1 ê°ì§€! Episode {episode+1} (ì—°ì† {consecutive_step1}íšŒ, ì´ {total_step1_count}íšŒ)")
            print("   â†’ ë°ì´í„° ë²„ë¦¼ (replay buffer ì €ì¥ ì•ˆí•¨)")
            
            # ì—°ì† 3íšŒ ì´ìƒì´ë©´ í™˜ê²½ ê°•ì œ ë¦¬ì…‹
            if consecutive_step1 >= 3:
                print(f"   â†’ ì—°ì† {consecutive_step1}íšŒ ë°œìƒ, í™˜ê²½ ê°•ì œ ì¬ì´ˆê¸°í™”")
                env = force_reset_environment(env, action_bounds)
                consecutive_step1 = 0
                time.sleep(1)  # ì•ˆì •í™” ëŒ€ê¸°
            
        else:
            consecutive_step1 = 0  # ì„±ê³µí•˜ë©´ ì—°ì† ì¹´ìš´í„° ë¦¬ì…‹
            
            print(f"Episode {episode+1:4d}: Steps={episode_steps:3d}, Reward={total_reward:7.2f}")
            
            # ì •ìƒ ë°ì´í„°ë§Œ ì €ì¥ ë° í•™ìŠµ
            for transition in episode_data:
                agent.store(transition)
                agent.train()

        # ì§§ì€ ì—í”¼ì†Œë“œ ë²„í¼ ì •ë¦¬ (ì •ìƒ ì—í”¼ì†Œë“œë§Œ ëŒ€ìƒ)
        if episode_steps > 1 and episode_steps < 10:
            if agent.replay_buffer.size() > 2000:
                agent.replay_buffer.clear_bad_patterns(threshold_reward=1.0)
                print(f"[INFO] ë²„í¼ ì •ë¦¬ ì™„ë£Œ")
        
        # 100 ì—í”¼ì†Œë“œë§ˆë‹¤ ì €ì¥
        if (episode + 1) % 100 == 0:
            agent.save_model(SAVE_DIR)
            print(f"[INFO] ëª¨ë¸ ì €ì¥ ì™„ë£Œ (Episode {episode+1})")
            print(f"[STATS] ì´ Step 1 ë°œìƒ: {total_step1_count}íšŒ")

    # ìµœì¢… ì €ì¥
    agent.save_model(SAVE_DIR)
    print(f"\n[FINAL] í•™ìŠµ ì™„ë£Œ! ì´ Step 1 ë°œìƒ: {total_step1_count}íšŒ")
    env.close()

if __name__ == "__main__":
    main()