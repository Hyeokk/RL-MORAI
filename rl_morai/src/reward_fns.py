import os
import sys
import numpy as np

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from gym_morai.envs.morai_sensor import MoraiSensor

class RewardFns:
    @staticmethod
    def adaptive_speed_lanefollow_reward(sensor, max_cte=0.8, max_speed=30.0):
        """
        ê³¡ì„ ì—ì„œëŠ” ì†ë„ë¥¼ ì¤„ì´ê³ , ì§ì„ ì—ì„œëŠ” ë¹ ë¥´ê²Œ ê°€ë„ë¡ ìœ ë„í•˜ëŠ” ë³´ìƒ í•¨ìˆ˜
        """
        last_steering = [0.0]
        last_cte = [None]
        
        def reward_fn(obs):
            cte = sensor.cal_cte()
            if cte is None:
                return 0.0
                
            velocity = sensor.get_velocity() or 0.0
            current_steering = getattr(sensor, 'last_steering', 0.0)
            steering_change = abs(current_steering - last_steering[0])
            
            # 1. ê¸°ë³¸ ì°¨ì„  ìœ ì§€ ë³´ìƒ
            lane_reward = (max_cte - cte) / max_cte if cte <= max_cte else 0.0
            
            # ğŸ”¥ 2. ì ì‘ì  ì†ë„ ë³´ìƒ (í•µì‹¬!)
            
            # ë°©ë²• 1: ì¡°í–¥ê° ê¸°ë°˜ ëª©í‘œ ì†ë„ ì¡°ì •
            abs_steering = abs(current_steering)
            if abs_steering < 0.1:  # ê±°ì˜ ì§ì„ 
                target_speed = max_speed * 0.9  # 90% ì†ë„
            elif abs_steering < 0.3:  # ì•½ê°„ì˜ ê³¡ì„ 
                target_speed = max_speed * 0.7  # 70% ì†ë„
            else:  # ê¸‰ê²©í•œ ê³¡ì„ 
                target_speed = max_speed * 0.5  # 50% ì†ë„
                
            # ëª©í‘œ ì†ë„ì— ë”°ë¥¸ ë³´ìƒ (ë¹ ë¥¼ìˆ˜ë¡ ì¢‹ë˜, ëª©í‘œ ì´ˆê³¼ ì‹œ í˜ë„í‹°)
            if velocity <= target_speed:
                speed_reward = velocity / target_speed  # 0~1.0
            else:
                speed_reward = max(0.0, 1.0 - (velocity - target_speed) / target_speed)
            
            # ë°©ë²• 2: CTE ë³€í™”ìœ¨ ê¸°ë°˜ ì†ë„ ì¡°ì • (ì¶”ê°€ ë³´ì™„)
            cte_change_penalty = 0.0
            if last_cte[0] is not None:
                cte_change = abs(cte - last_cte[0])
                if cte_change > 0.1 and velocity > max_speed * 0.6:  # CTEê°€ ë¹ ë¥´ê²Œ ë³€í•˜ëŠ”ë° ë¹ ë¥¸ ì†ë„
                    cte_change_penalty = -0.5
            last_cte[0] = cte
            
            # 3. ì¡°í–¥ ì•ˆì •ì„± ë³´ìƒ
            steering_reward = max(0.0, 1.0 - steering_change / 0.5)
            last_steering[0] = current_steering
            
            # ì´ ë³´ìƒ ê³„ì‚°
            total = (lane_reward * 1.5 + 
                    speed_reward * 1.0 + 
                    steering_reward * 0.5 + 
                    cte_change_penalty)
            
            return max(total, 0.0)
        
        return reward_fn

class CurriculumReward:
    @staticmethod
    def progressive_lane_reward(sensor, episode_counter=[0]):
        """
        í•™ìŠµ ë‹¨ê³„ì— ë”°ë¼ ë³´ìƒ êµ¬ì¡°ê°€ ë³€í™”í•˜ëŠ” í•¨ìˆ˜
        """
        
        step_count = [0]
        
        def get_reward_weights(episode_num):
            """ì—í”¼ì†Œë“œ ìˆ˜ì— ë”°ë¥¸ ë³´ìƒ ê°€ì¤‘ì¹˜ ì¡°ì •"""
            if episode_num < 200:
                # ì´ˆê¸‰: ìƒì¡´ê³¼ ê¸°ë³¸ ì£¼í–‰ì— ì§‘ì¤‘
                return {
                    'survival': 2.0,    # ìƒì¡´ ë³´ìƒ ë†’ìŒ
                    'cte': 1.0,         # CTE ë³´ìƒ ë³´í†µ
                    'speed': 0.5,       # ì†ë„ ë³´ìƒ ë‚®ìŒ
                    'precision': 0.0    # ì •ë°€ë„ ë³´ìƒ ì—†ìŒ
                }
            elif episode_num < 500:
                # ì¤‘ê¸‰: ì•ˆì •ì ì¸ ì°¨ì„  ìœ ì§€
                return {
                    'survival': 1.5,    # ìƒì¡´ ë³´ìƒ ê°ì†Œ
                    'cte': 2.0,         # CTE ë³´ìƒ ì¦ê°€
                    'speed': 1.0,       # ì†ë„ ë³´ìƒ ì¦ê°€
                    'precision': 0.5    # ì •ë°€ë„ ë³´ìƒ ì¶”ê°€
                }
            else:
                # ê³ ê¸‰: ì •ë°€í•œ ì°¨ì„  ì¤‘ì‹¬ ì£¼í–‰
                return {
                    'survival': 1.0,    # ìƒì¡´ ë³´ìƒ ê¸°ë³¸
                    'cte': 3.0,         # CTE ë³´ìƒ ìµœëŒ€
                    'speed': 1.5,       # ì†ë„ ë³´ìƒ ë†’ìŒ
                    'precision': 2.0    # ì •ë°€ë„ ë³´ìƒ ë†’ìŒ
                }
        
        def reward_fn(obs):
            step_count[0] += 1
            current_episode = episode_counter[0]
            weights = get_reward_weights(current_episode)
            
            # ê¸°ë³¸ ì„¼ì„œ ë°ì´í„°
            cte = sensor.cal_cte()
            if cte is None:
                return -1.0
            
            velocity = sensor.get_velocity() or 0.0
            
            # 1. ìƒì¡´ ë³´ìƒ
            survival_reward = weights['survival']
            
            # 2. CTE ë³´ìƒ (ë‹¨ê³„ë³„ ì¡°ì •)
            if current_episode < 200:
                # ì´ˆê¸‰: ê´€ëŒ€í•œ CTE ë³´ìƒ
                max_cte = 2.0
                cte_reward = max(0.0, (max_cte - cte) / max_cte) * weights['cte']
            elif current_episode < 500:
                # ì¤‘ê¸‰: í‘œì¤€ CTE ë³´ìƒ
                max_cte = 1.5
                cte_reward = max(0.0, (max_cte - cte) / max_cte) * weights['cte']
            else:
                # ê³ ê¸‰: ì—„ê²©í•œ CTE ë³´ìƒ
                max_cte = 1.0
                cte_reward = max(0.0, (max_cte - cte) / max_cte) * weights['cte']
            
            # 3. ì†ë„ ë³´ìƒ
            target_speed = 20.0
            speed_diff = abs(velocity - target_speed)
            speed_reward = max(0.0, 1.0 - speed_diff / target_speed) * weights['speed']
            
            # 4. ì •ë°€ë„ ë³´ìƒ (ê³ ê¸‰ ë‹¨ê³„ì—ì„œë§Œ)
            precision_reward = 0.0
            if weights['precision'] > 0:
                # ë§¤ìš° ì •í™•í•œ ì°¨ì„  ì¤‘ì‹¬ ì£¼í–‰ì— ëŒ€í•œ ë³´ë„ˆìŠ¤
                if cte < 0.3:  # 30cm ë¯¸ë§Œ
                    precision_reward = weights['precision']
                elif cte < 0.5:  # 50cm ë¯¸ë§Œ
                    precision_reward = weights['precision'] * 0.5
            
            # 5. ì¥ê¸° ìƒì¡´ ë³´ë„ˆìŠ¤
            longevity_bonus = min(1.0, step_count[0] / 200.0) * 0.5
            
            total_reward = (survival_reward + cte_reward + speed_reward + 
                           precision_reward + longevity_bonus)
            
            return total_reward
        
        return reward_fn