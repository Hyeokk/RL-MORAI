import os
import csv
import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from collections import deque, defaultdict
import json
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import time
import signal

try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    print("Warning: TensorBoard not available. Install with: pip install tensorboard")
    TENSORBOARD_AVAILABLE = False

# =============================================================================
# í™˜ê²½ ê´€ë¦¬
# =============================================================================
class EnvironmentManager:
    """MORAI í™˜ê²½ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def setup_environment(action_bounds):
        """í™˜ê²½ ì´ˆê¸°í™”"""
        try:
            from gym_morai.envs.morai_env import MoraiEnv
            from gym_morai.envs.reward_fns import RewardFns
            from gym_morai.envs.terminated_fns import TerminatedFns
        except ImportError:
            raise ImportError("MORAI environment is required")
        
        env = MoraiEnv(action_bounds=action_bounds)
        sensor = env.sensor
        
        # ë³´ìƒ ë° ì¢…ë£Œ í•¨ìˆ˜ ì„¤ì •
        env.set_reward_fn(RewardFns.adaptive_speed_lanefollow_reward(sensor))
        env.set_episode_over_fn(TerminatedFns.cte_done(sensor, max_cte=0.5))
        
        return env, sensor
    
    @staticmethod
    def force_reset_environment(env, action_bounds):
        """í™˜ê²½ ê°•ì œ ì¬ì´ˆê¸°í™”"""
        try:
            print("[RESET] í™˜ê²½ ê°•ì œ ì¬ì´ˆê¸°í™” ì¤‘...")
            env.close()
            time.sleep(2)
            
            new_env, new_sensor = EnvironmentManager.setup_environment(action_bounds)
            print("[RESET] í™˜ê²½ ì¬ì´ˆê¸°í™” ì™„ë£Œ")
            return new_env, new_sensor
        except Exception as e:
            print(f"[ERROR] í™˜ê²½ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return env, env.sensor

class EnvironmentLabelManager:
    """í™˜ê²½ ë¼ë²¨ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, manual_env=None):
        self.manual_env = manual_env
        self.env_name_to_label = {'solid': 0, 'dashed': 1, 'dash': 1, 'shadow': 2}
        self.env_label_to_name = {0: 'solid', 1: 'dashed', 2: 'shadow'}
        
        # í™˜ê²½ ê°ì§€ê¸°ëŠ” auto ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©
        if manual_env is None:
            try:
                from Multi_PPO import LaneEnvironmentDetector
                self.env_detector = LaneEnvironmentDetector()
            except ImportError:
                self.env_detector = None
        else:
            self.env_detector = None
        
    def get_environment_label(self, obs_dict, sensor=None, episode=None, step=None):
        """í™˜ê²½ ë¼ë²¨ íšë“"""
        if self.manual_env is not None:
            # ìˆ˜ë™ ì§€ì • ëª¨ë“œ
            return self.env_name_to_label.get(self.manual_env.lower(), 0)
        elif self.env_detector is not None:
            # ìë™ ê°ì§€ ëª¨ë“œ
            return self.env_detector.detect_lane_environment(obs_dict, sensor)
        else:
            # ê¸°ë³¸ê°’
            return 0
    
    def get_environment_name(self, env_label):
        """í™˜ê²½ ë¼ë²¨ì„ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        return self.env_label_to_name.get(env_label, 'unknown')
    
    def print_mode_info(self):
        """í˜„ì¬ í™˜ê²½ ëª¨ë“œ ì •ë³´ ì¶œë ¥"""
        if self.manual_env is not None:
            env_label = self.env_name_to_label.get(self.manual_env.lower(), 0)
            print(f"[ENV] ìˆ˜ë™ í™˜ê²½ ëª¨ë“œ: {self.manual_env} (ë¼ë²¨={env_label})")
            print(f"[ENV] í•´ë‹¹ í™˜ê²½ì˜ Criticë§Œ í•™ìŠµë©ë‹ˆë‹¤.")
        else:
            print(f"[ENV] ìë™ ê°ì§€ ëª¨ë“œ: ì´ë¯¸ì§€ ë¶„ì„ì„ í†µí•œ í™˜ê²½ ìë™ ë¶„ë¥˜")

# =============================================================================
# í•™ìŠµ í†µê³„ ê´€ë¦¬
# =============================================================================
class TrainingStats:
    """í•™ìŠµ í†µê³„ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.episode_rewards = []
        self.episode_lengths = []
        self.total_steps = 0
        self.total_short_episodes = 0
        self.total_step1_count = 0
        self.consecutive_short_episodes = 0
        self.env_distribution = {'solid': 0, 'dashed': 0, 'shadow': 0}
        self.env_names = ['solid', 'dashed', 'shadow']
        
    def add_episode(self, reward, length, env_label):
        """ì—í”¼ì†Œë“œ í†µê³„ ì¶”ê°€"""
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)
        self.env_distribution[self.env_names[env_label]] += 1
        
    def add_short_episode(self, length):
        """ì§§ì€ ì—í”¼ì†Œë“œ í†µê³„ ì¶”ê°€"""
        self.consecutive_short_episodes += 1
        self.total_short_episodes += 1
        if length == 1:
            self.total_step1_count += 1
    
    def reset_consecutive_count(self):
        """ì—°ì† ì§§ì€ ì—í”¼ì†Œë“œ ì¹´ìš´íŠ¸ ë¦¬ì…‹"""
        self.consecutive_short_episodes = 0
    
    def is_invalid_episode(self, episode_steps, min_steps=10):
        """ìœ íš¨í•˜ì§€ ì•Šì€ ì—í”¼ì†Œë“œ íŒë³„"""
        return episode_steps <= min_steps
    
    def print_stats(self, episode, manual_env=None):
        """í†µê³„ ì¶œë ¥"""
        if len(self.episode_rewards) > 0:
            recent_rewards = self.episode_rewards[-50:] if len(self.episode_rewards) >= 50 else self.episode_rewards
            avg_reward = np.mean(recent_rewards)
            avg_length = np.mean(self.episode_lengths[-50:]) if len(self.episode_lengths) >= 50 else np.mean(self.episode_lengths)
            
            print(f"[STATS] Episode {episode}: ìµœê·¼ í‰ê·  - Reward: {avg_reward:.2f}, Length: {avg_length:.1f}")
            print(f"[STATS] ì‹¤íŒ¨ ì—í”¼ì†Œë“œ: Step1={self.total_step1_count}, ì§§ì€ ì—í”¼ì†Œë“œ={self.total_short_episodes}")
            
            # í™˜ê²½ ë¶„í¬ ì¶œë ¥
            total_samples = sum(self.env_distribution.values())
            if total_samples > 0:
                for env_name, count in self.env_distribution.items():
                    ratio = count / total_samples
                    print(f"[STATS] {env_name}: {ratio:.2%}")
            
            # ìˆ˜ë™ í™˜ê²½ ëª¨ë“œ ì¶”ê°€ ì •ë³´
            if manual_env:
                print(f"[FOCUS] í˜„ì¬ í•™ìŠµ ì¤‘ì¸ í™˜ê²½: {manual_env}")

    def get_summary(self):
        """í•™ìŠµ ì™„ë£Œ í›„ ìš”ì•½ í†µê³„"""
        return {
            'total_episodes': len(self.episode_rewards),
            'success_episodes': len(self.episode_rewards),
            'failed_episodes': self.total_short_episodes,
            'step1_episodes': self.total_step1_count,
            'final_avg_reward': np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards) if self.episode_rewards else 0,
            'success_rate': len(self.episode_rewards) / (len(self.episode_rewards) + self.total_short_episodes) * 100 if (len(self.episode_rewards) + self.total_short_episodes) > 0 else 0,
            'env_distribution': self.env_distribution.copy()
        }

# =============================================================================
# í•™ìŠµ ì„¸ì…˜ ê´€ë¦¬ì
# =============================================================================
class TrainingSession:
    """ì „ì²´ í•™ìŠµ ì„¸ì…˜ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, manual_env=None, save_dir="/home/kuuve/catkin_ws/src/pt/", log_dir=None):
        self.manual_env = manual_env
        self.save_dir = save_dir
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        if log_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
            env_suffix = f"_{manual_env}" if manual_env else ""
            self.log_dir = f"/home/kuuve/catkin_ws/src/logs/multi_critic{env_suffix}_{timestamp}"
        else:
            self.log_dir = log_dir
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.env_manager = EnvironmentLabelManager(manual_env)
        self.stats = TrainingStats()
        self.tb_logger = None
        self.performance_analyzer = None
        
        # ì¢…ë£Œ í”Œë˜ê·¸ ì„¤ì •
        self.stop_flag = False
        signal.signal(signal.SIGINT, self._signal_handler)
        
    def _signal_handler(self, sig, frame):
        """í‚¤ë³´ë“œ ì¸í„°ëŸ½íŠ¸ í•¸ë“¤ëŸ¬"""
        self.stop_flag = True
        print("\ní•™ìŠµ ì¤‘ë‹¨ ì‹ í˜¸ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤. ì•ˆì „í•˜ê²Œ ì¢…ë£Œ ì¤‘...")
        
    def setup_logging(self, agent):
        """ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if TENSORBOARD_AVAILABLE:
            experiment_name = f"multi_critic_ppo"
            if self.manual_env:
                experiment_name += f"_{self.manual_env}"
            
            self.tb_logger = TensorBoardLogger(self.log_dir, experiment_name)
            self.performance_analyzer = PerformanceAnalyzer(self.tb_logger)
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
            hparams = agent.get_hyperparameters()
            hparams['manual_environment'] = self.manual_env or 'auto_detection'
            self.tb_logger.log_hyperparameters(hparams)
            
            print(f"ğŸ“Š TensorBoard: tensorboard --logdir {self.log_dir}")
        else:
            print("âš ï¸  TensorBoard ë¯¸ì„¤ì¹˜ - ê¸°ë³¸ ë¡œê¹…ë§Œ ì‚¬ìš©")
    
    def log_training_metrics(self, global_step, train_metrics, manual_env=None):
        """í•™ìŠµ ì§€í‘œ ë¡œê¹…"""
        if self.tb_logger and train_metrics:
            self.tb_logger.log_training_losses(global_step, train_metrics)
            self.tb_logger.log_classifier_metrics(global_step, train_metrics['classifier_accuracy'])
            
            # í™˜ê²½ë³„ í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹…
            if manual_env:
                env_label = self.env_manager.env_name_to_label.get(manual_env.lower(), 0)
                self.tb_logger.log_custom_metric(f'Training/Focus_Environment', env_label, global_step)
                self.tb_logger.log_custom_metric(f'Training/{manual_env}_Critic_Loss', 
                                              train_metrics['critic_loss'], global_step)
    
    def log_episode_metrics(self, episode, reward, length, env_label, manual_env=None):
        """ì—í”¼ì†Œë“œ ì§€í‘œ ë¡œê¹…"""
        if self.tb_logger:
            self.tb_logger.log_episode_metrics(episode, reward, length)
            
            # í™˜ê²½ë³„ ì„±ëŠ¥ ë¡œê¹…
            env_name = self.env_manager.get_environment_name(env_label)
            self.tb_logger.log_custom_metric(f'Performance/Reward_{env_name}', reward, episode)
            
            if manual_env:
                self.tb_logger.log_custom_metric(f'Performance/Reward_Focus_{manual_env}', reward, episode)
        
        # ì„±ëŠ¥ ë¶„ì„ ë°ì´í„° ì¶”ê°€
        if self.performance_analyzer:
            episode_env_dist = {name: 0 for name in self.stats.env_names}
            episode_env_dist[self.stats.env_names[env_label]] = 1
            self.performance_analyzer.add_episode_data(episode, reward, length, episode_env_dist, 0.0)
    
    def should_stop(self):
        """í•™ìŠµ ì¤‘ë‹¨ ì—¬ë¶€ í™•ì¸"""
        return self.stop_flag
    
    def get_save_path(self, final=False):
        """ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë°˜í™˜"""
        if self.manual_env:
            suffix = "final_model" if final else "model"
            return os.path.join(self.save_dir, f"{suffix}_{self.manual_env}")
        else:
            return self.save_dir
    
    def analyze_performance(self):
        """ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
        if self.performance_analyzer:
            self.performance_analyzer.analyze_convergence()
            self.performance_analyzer.analyze_environment_adaptation()
    
    def print_experiment_info(self):
        """ì‹¤í—˜ ì •ë³´ ì¶œë ¥"""
        print("Multi-Critic PPO ê°•í™”í•™ìŠµ ì‹œì‘")
        print("=" * 60)
        
        if self.manual_env:
            print(f"ğŸ¯ í™˜ê²½ ì§€ì • ëª¨ë“œ: {self.manual_env.upper()}")
            env_label = self.env_manager.env_name_to_label.get(self.manual_env.lower(), 0)
            print(f"   â†’ Critic_{env_label} ì „ìš© í•™ìŠµ")
            print(f"   â†’ ë‹¤ë¥¸ í™˜ê²½ì˜ Criticì€ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        else:
            print(f"ğŸ” ìë™ ê°ì§€ ëª¨ë“œ: ì´ë¯¸ì§€ ë¶„ì„ì„ í†µí•œ í™˜ê²½ ìë™ ë¶„ë¥˜")
            print(f"   â†’ ëª¨ë“  Criticì´ ìƒí™©ì— ë”°ë¼ í•™ìŠµë©ë‹ˆë‹¤")
        
        print("=" * 60)
    
    def print_final_summary(self, episode_count):
        """ìµœì¢… í•™ìŠµ ê²°ê³¼ ìš”ì•½"""
        summary = self.stats.get_summary()
        
        print("\nMulti-Critic PPO í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì—í”¼ì†Œë“œ: {episode_count}")
        print(f"ì •ìƒ ì—í”¼ì†Œë“œ: {summary['success_episodes']}ê°œ")
        print(f"ì‹¤íŒ¨ ì—í”¼ì†Œë“œ: {summary['failed_episodes']}ê°œ (Step1: {summary['step1_episodes']})")
        print(f"ìµœì¢… í‰ê·  ë³´ìƒ: {summary['final_avg_reward']:.2f}")
        print(f"ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        
        if self.manual_env:
            env_label = self.env_manager.env_name_to_label.get(self.manual_env.lower(), 0)
            print(f"[FOCUS] í•™ìŠµëœ í™˜ê²½: {self.manual_env} (Critic_{env_label})")
            print(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {self.get_save_path(final=True)}")
        
        if self.tb_logger:
            print(f"ğŸ“Š TensorBoard: tensorboard --logdir {self.log_dir}")
    
    def cleanup(self, env):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.tb_logger:
            self.tb_logger.close()
        if env:
            env.close()

# =============================================================================
# ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤
# =============================================================================
class Cal_CTE:
    @staticmethod
    def load_centerline(csv_path):
        x_list = []
        y_list = []
        with open(csv_path, newline='') as csvfile:
            reader = csv.reader(csvfile)
            next(reader)  # í—¤ë” ìŠ¤í‚µ
            for row in reader:
                x = float(row[1])  # 2ì—´: x
                y = float(row[2])  # 3ì—´: y
                x_list.append(x)
                y_list.append(y)
        return np.array(list(zip(x_list, y_list)))  # (N, 2) array

    @staticmethod
    def calculate_cte(agent_pos, path_points):
        agent_pos = np.array(agent_pos)  # tuple -> ndarray
        distances = np.linalg.norm(path_points - agent_pos, axis=1)
        min_index = np.argmin(distances)
        closest_point = path_points[min_index]
        cte = np.linalg.norm(agent_pos - closest_point)
        return cte

class ConfigManager:
    """ì„¤ì • ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def save_config(config_dict: Dict, save_path: str):
        """ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        # Convert any non-serializable objects to strings
        serializable_config = {}
        for key, value in config_dict.items():
            try:
                json.dumps(value)  # Test if serializable
                serializable_config[key] = value
            except TypeError:
                serializable_config[key] = str(value)
        
        with open(save_path, 'w') as f:
            json.dump(serializable_config, f, indent=4)
    
    @staticmethod
    def load_config(load_path: str) -> Dict:
        """JSON íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ"""
        with open(load_path, 'r') as f:
            return json.load(f)

class TensorBoardLogger:
    """
    Multi-Critic PPOë¥¼ ìœ„í•œ TensorBoard ë¡œê¹… í´ë˜ìŠ¤
    ë…¼ë¬¸ì˜ í‰ê°€ ì§€í‘œë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ê¸°ë¡
    """
    
    def __init__(self, log_dir: str, experiment_name: str = None):
        if not TENSORBOARD_AVAILABLE:
            raise ImportError("TensorBoard is not available. Install with: pip install tensorboard")
            
        if experiment_name is None:
            experiment_name = f"multi_critic_ppo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
        self.log_dir = os.path.join(log_dir, experiment_name)
        os.makedirs(self.log_dir, exist_ok=True)
        
        self.writer = SummaryWriter(self.log_dir)
        self.experiment_name = experiment_name
        
        # ì´ë™ í‰ê·  ê³„ì‚°ì„ ìœ„í•œ ë²„í¼
        self.reward_buffer = deque(maxlen=100)
        self.step_counter = 0
        self.episode_counter = 0
        
        # Critic ì„ íƒ ì˜¤ë¥˜ìœ¨ ì¶”ì 
        self.critic_selection_errors = deque(maxlen=1000)
        
    def log_episode_metrics(self, episode: int, episode_reward: float, episode_length: int = None):
        """ì—í”¼ì†Œë“œë³„ ì§€í‘œ ë¡œê¹…"""
        self.episode_counter = episode
        self.reward_buffer.append(episode_reward)
        
        # Reward/Episode - ì •ì±…ì˜ ì „ì²´ ì„±ëŠ¥ ì¶”ì´ í™•ì¸
        self.writer.add_scalar('Reward/Episode', episode_reward, episode)
        
        # Reward/MovingAverage - ìµœê·¼ ì—í”¼ì†Œë“œ í‰ê·  ë³´ìƒ (ì‹œê°ì  ì¶”ì„¸ í™•ì¸ìš©)
        if len(self.reward_buffer) >= 10:  # ìµœì†Œ 10ê°œ ì—í”¼ì†Œë“œ í›„ë¶€í„°
            moving_avg = np.mean(list(self.reward_buffer))
            self.writer.add_scalar('Reward/MovingAverage', moving_avg, episode)
        
        # Episode length tracking
        if episode_length is not None:
            self.writer.add_scalar('Episode/Length', episode_length, episode)
            
    def log_training_losses(self, step: int, loss_dict: Dict[str, float]):
        """í•™ìŠµ ê³¼ì •ì˜ ì†ì‹¤ í•¨ìˆ˜ë“¤ ë¡œê¹…"""
        self.step_counter = step
        
        # Loss/Actor - actor(policy)ì˜ í•™ìŠµ ìˆ˜ë ´ ì¶”ì 
        if 'actor_loss' in loss_dict:
            self.writer.add_scalar('Loss/Actor', loss_dict['actor_loss'], step)
        
        # Loss/Critic_{env} - criticì˜ í™˜ê²½ë³„ í•™ìŠµ ìƒíƒœ ì¶”ì 
        if 'critic_losses' in loss_dict:
            for env_name, critic_loss in loss_dict['critic_losses'].items():
                self.writer.add_scalar(f'Loss/Critic_{env_name}', critic_loss, step)
        
        # Total critic loss
        if 'critic_loss' in loss_dict:
            self.writer.add_scalar('Loss/Critic_Total', loss_dict['critic_loss'], step)
        
        # Loss/Classifier - í™˜ê²½ ë¶„ë¥˜ê¸°ì˜ í•™ìŠµ ìˆ˜ë ´ í™•ì¸
        if 'classifier_loss' in loss_dict:
            self.writer.add_scalar('Loss/Classifier', loss_dict['classifier_loss'], step)
        
        # Total loss
        if 'total_loss' in loss_dict:
            self.writer.add_scalar('Loss/Total', loss_dict['total_loss'], step)
            
        # Entropy loss
        if 'entropy' in loss_dict:
            self.writer.add_scalar('Loss/Entropy', loss_dict['entropy'], step)
    
    def log_classifier_metrics(self, step: int, accuracy: float, predictions: np.ndarray = None, 
                             ground_truth: np.ndarray = None):
        """ë¶„ë¥˜ê¸° ì„±ëŠ¥ ì§€í‘œ ë¡œê¹…"""
        
        # Accuracy/Classifier - í™˜ê²½ ë¶„ë¥˜ê¸° ì •í™•ë„ í‰ê°€ (validation ê¸°ì¤€)
        self.writer.add_scalar('Accuracy/Classifier', accuracy, step)
        
        # í˜¼ë™ í–‰ë ¬ ë° í´ë˜ìŠ¤ë³„ ì •í™•ë„
        if predictions is not None and ground_truth is not None:
            # í´ë˜ìŠ¤ë³„ ì •í™•ë„
            for class_idx in range(3):  # solid, dashed, shadow
                class_mask = (ground_truth == class_idx)
                if np.any(class_mask):
                    class_accuracy = np.mean(predictions[class_mask] == ground_truth[class_mask])
                    class_names = ['solid', 'dashed', 'shadow']
                    self.writer.add_scalar(f'Accuracy/Classifier_{class_names[class_idx]}', 
                                         class_accuracy, step)
    
    def log_custom_metric(self, name: str, value: float, step: int):
        """ì‚¬ìš©ì ì •ì˜ ì§€í‘œ ë¡œê¹…"""
        self.writer.add_scalar(name, value, step)
    
    def log_hyperparameters(self, hparams: Dict[str, any], metrics: Dict[str, float] = None):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…"""
        if metrics is None:
            metrics = {'placeholder': 0.0}  # TensorBoard requires at least one metric
            
        self.writer.add_hparams(hparams, metrics)
    
    def close(self):
        """TensorBoard writer ì¢…ë£Œ"""
        if hasattr(self, 'writer'):
            self.writer.close()

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ì„ ìœ„í•œ ê³ ê¸‰ ìœ í‹¸ë¦¬í‹°"""
    
    def __init__(self, tb_logger: TensorBoardLogger):
        self.tb_logger = tb_logger
        self.episode_data = []
        
    def add_episode_data(self, episode: int, reward: float, length: int, 
                        env_distribution: Dict[str, int], avg_cte: float):
        """ì—í”¼ì†Œë“œ ë°ì´í„° ìˆ˜ì§‘"""
        self.episode_data.append({
            'episode': episode,
            'reward': reward,
            'length': length,
            'env_distribution': env_distribution,
            'avg_cte': avg_cte
        })
    
    def analyze_convergence(self, window_size: int = 100):
        """ìˆ˜ë ´ì„± ë¶„ì„"""
        if len(self.episode_data) < window_size:
            return
            
        recent_rewards = [ep['reward'] for ep in self.episode_data[-window_size:]]
        
        # ë¶„ì‚° ë¶„ì„
        reward_variance = np.var(recent_rewards)
        self.tb_logger.log_custom_metric('Analysis/RewardVariance', reward_variance, 
                                       self.episode_data[-1]['episode'])
        
        # íŠ¸ë Œë“œ ë¶„ì„
        x = np.arange(len(recent_rewards))
        slope, _, r_value, _, _ = stats.linregress(x, recent_rewards)
        self.tb_logger.log_custom_metric('Analysis/RewardTrend', slope, 
                                       self.episode_data[-1]['episode'])
        self.tb_logger.log_custom_metric('Analysis/TrendCorrelation', r_value, 
                                       self.episode_data[-1]['episode'])
    
    def analyze_environment_adaptation(self):
        """í™˜ê²½ë³„ ì ì‘ì„± ë¶„ì„"""
        if len(self.episode_data) < 50:
            return
            
        # í™˜ê²½ë³„ ì„±ëŠ¥ ë¶„ì„
        env_performance = {'solid': [], 'dashed': [], 'shadow': []}
        
        for ep_data in self.episode_data[-50:]:  # ìµœê·¼ 50 ì—í”¼ì†Œë“œ
            total_samples = sum(ep_data['env_distribution'].values())
            if total_samples > 0:
                for env_name in env_performance.keys():
                    env_ratio = ep_data['env_distribution'].get(env_name, 0) / total_samples
                    if env_ratio > 0.3:  # í•´ë‹¹ í™˜ê²½ì´ 30% ì´ìƒì¸ ì—í”¼ì†Œë“œë§Œ
                        env_performance[env_name].append(ep_data['reward'])
        
        # í™˜ê²½ë³„ í‰ê·  ì„±ëŠ¥ ë¡œê¹…
        episode = self.episode_data[-1]['episode']
        for env_name, rewards in env_performance.items():
            if rewards:
                avg_performance = np.mean(rewards)
                self.tb_logger.log_custom_metric(f'Analysis/Performance_{env_name}', 
                                               avg_performance, episode)

# =============================================================================
# ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°
# =============================================================================
class Plot:
    @staticmethod
    def save_reward_csv(reward_list, csv_path):
        with open(csv_path, mode='w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["Episode", "Reward"])
            for i, r in enumerate(reward_list, start=1):
                writer.writerow([i, r])

    @staticmethod
    def plot_rewards(reward_list, save_path=None):
        plt.figure(figsize=(10, 5))
        plt.plot(reward_list, label="Total Reward")
        plt.xlabel("Episode")
        plt.ylabel("Reward")
        plt.title("Training Reward Curve")
        plt.grid(True)
        plt.legend()
        if save_path:
            plt.savefig(save_path)
            plt.close()
        else:
            plt.show()